{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBgNOgHRS1FW"
      },
      "source": [
        "# Flight Fare Prediction Project\n",
        "1. As first step, we load our `FlightFare_Dataset` from Project Directory, using Pandas `read_excel` method\n",
        "2. Then, we perform Feature Exploration and Engineering to transform our dataset\n",
        "3. Once done, we use a Feature Selection technique to select the most important features\n",
        "4. At this point, we train a Random Forest Regressor Model\n",
        "5. As next step, we do hyper-parameter tuning (using `RandomGridSearch`) to build the best model\n",
        "6. Finally, we export Model `.pkl` file back to Project Directory\n",
        "7. Towards the end, we proceed to Model Deployment step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duacQMllNqSn"
      },
      "source": [
        "## Set up Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vcrlrO11S1FZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YEDd3kzaOMCw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "b8b2d29a-5f62-4db9-a1e8-b22cf8c2fae9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-74feb274a128>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive - applicable, if working on Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Data_Train.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0mnormed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must either be a directory or not exist"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive - applicable, if working on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Data_Train.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PzSLmcHNwSe"
      },
      "outputs": [],
      "source": [
        "# Set Working Directory - if working on Google Drive\n",
        "%cd /content/drive/MyDrive/Project10_FlightPricePrediction\n",
        "\n",
        "# # Set Working Directory - if working on Local Machine\n",
        "# import os\n",
        "# os.chdir('/Users//replace_me')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFGnop_jS1Fa"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hbzcmp5S1Fb"
      },
      "outputs": [],
      "source": [
        "# Load dataset from Project folder\n",
        "dataset = pd.read_excel(\"/content/Data_Train.xlsx\")\n",
        "\n",
        "# To stretch head function output to the notebook width\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slLEP_6QS1Fc"
      },
      "outputs": [],
      "source": [
        "dataset.info()       # Print Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ABOEWcvUs4"
      },
      "source": [
        "### Check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nounGdRWS1Fd"
      },
      "outputs": [],
      "source": [
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkesT0mPS1Fd"
      },
      "outputs": [],
      "source": [
        "dataset.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLY6bzKoQUij"
      },
      "outputs": [],
      "source": [
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3gT5bY7S1Fe"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJFAzCADQ7lT"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Mr_4zVYCd5"
      },
      "source": [
        "### Handling Object Data\n",
        "**`Date_of_Journey`, `Dep_Time`, `Arrival_Time`, `Duration` are object datatype.** To derive numeric features on these, we use pandas `to_datetime` method to convert object data type to datetime datatype.\n",
        "\n",
        "<span style=\"color: red;\">Attribute `.dt.day`  will extract day from the date</span>\\\n",
        "<span style=\"color: red;\">Attribute `.dt.month` will extract  month from that date</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biASfcHxS1Fe"
      },
      "outputs": [],
      "source": [
        "# Date_of_Journey is the day when plane departs. \n",
        "dataset[\"journey_day\"] = pd.to_datetime(dataset.Date_of_Journey, format=\"%d/%m/%Y\").dt.day\n",
        "dataset[\"journey_month\"] = pd.to_datetime(dataset[\"Date_of_Journey\"], format = \"%d/%m/%Y\").dt.month\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT6s-eEBS1Ff"
      },
      "outputs": [],
      "source": [
        "# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n",
        "dataset.drop([\"Date_of_Journey\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4dI5r9pS1Ff"
      },
      "outputs": [],
      "source": [
        "# Departure time is when a plane leaves the gate. \n",
        "# Similar to Date_of_Journey we can extract values from Dep_Time\n",
        "# Extracting Hours\n",
        "dataset[\"dep_hour\"] = pd.to_datetime(dataset[\"Dep_Time\"]).dt.hour\n",
        "# Extracting Minutes\n",
        "dataset[\"dep_min\"] = pd.to_datetime(dataset[\"Dep_Time\"]).dt.minute\n",
        "# Now we drop Dep_Time as it is of no use\n",
        "dataset.drop([\"Dep_Time\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D9PH9U7S1Fg"
      },
      "outputs": [],
      "source": [
        "# Arrival time is when the plane pulls up to the gate.\n",
        "# Similar to Date_of_Journey we can extract values from Arrival_Time\n",
        "\n",
        "# Extracting Hours\n",
        "dataset[\"arrival_hour\"] = pd.to_datetime(dataset[\"Arrival_Time\"]).dt.hour\n",
        "# Extracting Minutes\n",
        "dataset[\"arrival_min\"] = pd.to_datetime(dataset[\"Arrival_Time\"]).dt.minute\n",
        "# Now we can drop Arrival_Time as it is of no use\n",
        "dataset.drop([\"Arrival_Time\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEhqWAvuS1Fg"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JqtBNwAS1Fg"
      },
      "outputs": [],
      "source": [
        "# Duration is the time taken by plane to reach destination\n",
        "# It is the difference betwen Arrival Time and Departure time\n",
        "# Assigning and converting Duration column into list, for looping through\n",
        "duration = list(dataset[\"Duration\"])\n",
        "# In table above, Row Index=2, we have Duration = 19h (missing minutes)\n",
        "# Looping through all duration values, to ensure it has both hours & mins: 'xh ym'\n",
        "for i in range(len(duration)):\n",
        "    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n",
        "        if \"h\" in duration[i]:\n",
        "            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n",
        "        else:\n",
        "            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n",
        "# Prepare separate duration_hours and duration_mins lists\n",
        "duration_hours = []\n",
        "duration_mins = []\n",
        "for i in range(len(duration)):\n",
        "    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n",
        "    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n",
        "\n",
        "# Add duration_hours and duration_mins list to our dataset df\n",
        "dataset[\"Duration_hours\"] = duration_hours\n",
        "dataset[\"Duration_mins\"] = duration_mins\n",
        "# Drop Duration column from the dataset\n",
        "dataset.drop([\"Duration\"], axis = 1, inplace = True)\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-A9XYVMS1Fh"
      },
      "source": [
        "### Handling Categorical Data\n",
        "\n",
        "**`Airline`, `Source`, `Destination`, `Route`, `Total_Stops`, `Additional_Info` are all categorical.** One can find many ways to handle categorical data, like:\n",
        "1. <span style=\"color: blue;\">**Nominal data**</span> --> data is not in any order --> <span style=\"color: green;\">**OneHotEncoder**</span> is used in this case\n",
        "2. <span style=\"color: blue;\">**Ordinal data**</span> --> data is in order --> <span style=\"color: green;\">**LabelEncoder**</span> is used in this case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUQOO8irS1Fh"
      },
      "outputs": [],
      "source": [
        "# Feature engineering on: Airline\n",
        "dataset[\"Airline\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkJgriADe4jh"
      },
      "outputs": [],
      "source": [
        "# As Airline is Nominal Categorical data we will perform OneHotEncoding\n",
        "Airline = dataset[[\"Airline\"]]\n",
        "Current_Airline_List = Airline['Airline']\n",
        "New_Airline_List = []\n",
        "\n",
        "for carrier in Current_Airline_List:\n",
        "  if carrier in ['Jet Airways', 'IndiGo', 'Air India', 'SpiceJet',\n",
        "       'Multiple carriers', 'GoAir', 'Vistara', 'Air Asia']:\n",
        "    New_Airline_List.append(carrier)\n",
        "  else:\n",
        "    New_Airline_List.append('Other')\n",
        "\n",
        "Airline['Airline'] = pd.DataFrame(New_Airline_List)\n",
        "Airline['Airline'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soPl0ATSS1Fh"
      },
      "outputs": [],
      "source": [
        "Airline = pd.get_dummies(Airline, drop_first= True)\n",
        "Airline.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPeuBE9OS1Fi"
      },
      "outputs": [],
      "source": [
        "# Feature engineering on: Source\n",
        "dataset[\"Source\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJggB_CdS1Fi"
      },
      "outputs": [],
      "source": [
        "# As Source is Nominal Categorical data we will perform OneHotEncoding\n",
        "Source = dataset[[\"Source\"]]\n",
        "Source = pd.get_dummies(Source, drop_first= True) \n",
        "# drop_first= True means we drop the first column to prevent multicollinearity\n",
        "Source.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvrboCFCS1Fi"
      },
      "outputs": [],
      "source": [
        "# Feature engineering on: Destination\n",
        "dataset[\"Destination\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEUP2BEO2TBq"
      },
      "outputs": [],
      "source": [
        "# Renaming destination 'New Delhi' to 'Delhi' - to match with Source\n",
        "Destination = dataset[[\"Destination\"]]\n",
        "Current_Destination_List = Destination['Destination']\n",
        "New_Destination_List = []\n",
        "\n",
        "for value in Current_Destination_List:\n",
        "  if value in ['New Delhi']:\n",
        "    New_Destination_List.append('Delhi')\n",
        "  else:\n",
        "    New_Destination_List.append(value)\n",
        "\n",
        "Destination['Destination'] = pd.DataFrame(New_Destination_List)\n",
        "\n",
        "# As Destination is Nominal Categorical data we will perform OneHotEncoding\n",
        "Destination = pd.get_dummies(Destination, drop_first = True)\n",
        "Destination.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg2gE6rxS1Fi"
      },
      "outputs": [],
      "source": [
        "# Additional_Info contains almost 80% no_info\n",
        "# Route and Total_Stops are related to each other\n",
        "dataset.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMiL6MZ4S1Fi"
      },
      "outputs": [],
      "source": [
        "# Feature engineering on: Total_Stops\n",
        "dataset[\"Total_Stops\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myZ0fz8LS1Fi"
      },
      "outputs": [],
      "source": [
        "# As this is case of Ordinal Categorical type we perform LabelEncoder\n",
        "# Here Values are assigned with corresponding keys\n",
        "dataset.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_G3ZVrXS1Fj"
      },
      "outputs": [],
      "source": [
        "# Concatenate dataframe --> train_data + Airline + Source + Destination\n",
        "data_train = pd.concat([dataset, Airline, Source, Destination], axis = 1) # axis = 1 signifies column\n",
        "data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n",
        "\n",
        "data_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOytf6t3S1Fj"
      },
      "outputs": [],
      "source": [
        "data_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEP5wMxQS1Fk"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "Finding out the best feature which will contribute and have good relation with target variable.\n",
        "Following are some of the feature selection methods:\n",
        "\n",
        "\n",
        "1. <span style=\"color: purple;\">**feature_importance_**</span>\n",
        "2. <span style=\"color: purple;\">**VIF**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8jHpbjSS1Fk"
      },
      "outputs": [],
      "source": [
        "data_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAWy6A20S1Fk"
      },
      "outputs": [],
      "source": [
        "X = data_train.loc[:, ['Total_Stops', 'journey_day', 'journey_month', 'dep_hour',\n",
        "       'dep_min', 'arrival_hour', 'arrival_min', 'Duration_hours',\n",
        "       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n",
        "       'Airline_Jet Airways', 'Airline_Multiple carriers', 'Airline_Other',\n",
        "       'Airline_SpiceJet', 'Airline_Vistara', 'Source_Chennai', 'Source_Delhi',\n",
        "       'Source_Kolkata', 'Source_Mumbai', 'Destination_Cochin',\n",
        "       'Destination_Delhi', 'Destination_Hyderabad', 'Destination_Kolkata']]\n",
        "y = data_train.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73m5ybQW5qzD"
      },
      "outputs": [],
      "source": [
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeS_DjXc5ayE"
      },
      "source": [
        "### feature_importance_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R2HKxjwS1Fk"
      },
      "outputs": [],
      "source": [
        "# Important feature using ExtraTreesRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "selection = ExtraTreesRegressor()\n",
        "selection.fit(X, y)\n",
        "\n",
        "print(selection.feature_importances_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDqTCwDTS1Fl"
      },
      "outputs": [],
      "source": [
        "#plot graph of feature importances for better visualization\n",
        "plt.figure(figsize = (12,8))\n",
        "feat_importances = pd.Series(selection.feature_importances_, index=X.columns)\n",
        "feat_importances.nlargest(25).plot(kind='barh')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiqMNCUH50LT"
      },
      "source": [
        "### VIF - Multicollinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0vbxppVmuFr"
      },
      "outputs": [],
      "source": [
        "# Checking for Multicollinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(z):\n",
        "    # Calculating Variable Inflation Factor (VIF)\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = z.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(z.values, i) for i in range(z.shape[1])]\n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbQuyvAamuwp"
      },
      "outputs": [],
      "source": [
        "# Compute VIF on X\n",
        "calc_vif(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEejlvy4rjhI"
      },
      "outputs": [],
      "source": [
        "# Drop 'Source_Delhi'\n",
        "X = data_train.loc[:, ['Total_Stops', 'journey_day', 'journey_month', 'dep_hour',\n",
        "       'dep_min', 'arrival_hour', 'arrival_min', 'Duration_hours',\n",
        "       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n",
        "       'Airline_Jet Airways', 'Airline_Multiple carriers', 'Airline_Other',\n",
        "       'Airline_SpiceJet', 'Airline_Vistara', 'Source_Chennai',\n",
        "       'Source_Kolkata', 'Source_Mumbai', 'Destination_Cochin',\n",
        "       'Destination_Delhi', 'Destination_Hyderabad', 'Destination_Kolkata']]\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpdCnWZJS1Fl"
      },
      "source": [
        "## Fit model - Random Forest\n",
        "\n",
        "1. Split dataset into train and test set in order to predict, w.r.t, X_test\n",
        "2. If needed do scaling of data\n",
        "    * Scaling is not required in Random forest\n",
        "3. Train Model\n",
        "4. Gauge Model Performance\n",
        "5. In regression check **RSME** Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot0B6DehS1Fl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmuPTH6nS1Fl"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_reg = RandomForestRegressor()\n",
        "rf_reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txn0DDgf8Da-"
      },
      "source": [
        "### Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qykQeIpsS1Fl"
      },
      "outputs": [],
      "source": [
        "print('Model Performance on Training Set:', round(rf_reg.score(X_train, y_train)*100,2))\n",
        "print('Model Performance on Test Set:', round(rf_reg.score(X_test, y_test)*100,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0fLHmzsS1Fl"
      },
      "outputs": [],
      "source": [
        "# Plot performance graph\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "plt.scatter(y_test, y_pred, alpha = 0.5)\n",
        "plt.xlabel(\"y_test\")\n",
        "plt.ylabel(\"y_pred\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8WI7In0S1Fl"
      },
      "outputs": [],
      "source": [
        "# Model Error Values\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "#RMSE = sqrt((PV-OV)^2/n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb7sNq18S1Fm"
      },
      "outputs": [],
      "source": [
        "# RMSE/(max(DV)-min(DV))\n",
        "print('Normalized RMSE ', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred))/(max(y_test)-min(y_test)),2))\n",
        "print('Max Value: ', max(y), '\\nMin Value: ', min(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvYeHQpcS1Fn"
      },
      "source": [
        "### Save the model .pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4ggjhPHS1Fn"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# open a file, where you ant to store the data\n",
        "file = open('c2_flight_rf.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(rf_reg, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfw9ROxQs1HM"
      },
      "source": [
        "# Prediction on Unseen data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zISCE_BYuh27"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "path = 'c1_flight_rf.pkl'\n",
        "model = open(path,'rb')\n",
        "rf_model = pickle.load(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnUogtuHs4T6"
      },
      "outputs": [],
      "source": [
        "unseen_dataset = pd.read_excel(\"./a2_Unseen_Dataset.xlsx\")\n",
        "unseen_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60tQPbrftMIY"
      },
      "outputs": [],
      "source": [
        "# Perform feature engineering on object dt variables\n",
        "# Feature Engineering on: 'Date_of_Journey'\n",
        "unseen_dataset[\"journey_day\"] = pd.to_datetime(unseen_dataset.Date_of_Journey, format=\"%d/%m/%Y\").dt.day\n",
        "unseen_dataset[\"journey_month\"] = pd.to_datetime(unseen_dataset[\"Date_of_Journey\"], format = \"%d/%m/%Y\").dt.month\n",
        "unseen_dataset.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n",
        "\n",
        "# Feature Engineering on: 'Dep_Time'\n",
        "unseen_dataset[\"dep_hour\"] = pd.to_datetime(unseen_dataset[\"Dep_Time\"]).dt.hour\n",
        "unseen_dataset[\"dep_min\"] = pd.to_datetime(unseen_dataset[\"Dep_Time\"]).dt.minute\n",
        "unseen_dataset.drop([\"Dep_Time\"], axis = 1, inplace = True)\n",
        "\n",
        "# Feature Engineering on: 'Arrival_Time'\n",
        "unseen_dataset[\"arrival_hour\"] = pd.to_datetime(unseen_dataset[\"Arrival_Time\"]).dt.hour\n",
        "unseen_dataset[\"arrival_min\"] = pd.to_datetime(unseen_dataset[\"Arrival_Time\"]).dt.minute\n",
        "unseen_dataset.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n",
        "\n",
        "# Feature Engineering on: 'Duration'\n",
        "duration = list(unseen_dataset[\"Duration\"])\n",
        "for i in range(len(duration)):\n",
        "    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n",
        "        if \"h\" in duration[i]:\n",
        "            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n",
        "        else:\n",
        "            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n",
        "duration_hours = []\n",
        "duration_mins = []\n",
        "for i in range(len(duration)):\n",
        "    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n",
        "    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n",
        "unseen_dataset[\"Duration_hours\"] = duration_hours\n",
        "unseen_dataset[\"Duration_mins\"] = duration_mins\n",
        "unseen_dataset.drop([\"Duration\"], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# Perform feature engineering on Categorical dt variables\n",
        "# Feature Engineering on: 'Airline'\n",
        "Airline = unseen_dataset[[\"Airline\"]]\n",
        "New_Airline_List = []\n",
        "Current_Airline_List = Airline['Airline']\n",
        "for carrier in Current_Airline_List:\n",
        "  if carrier in ['IndiGo', 'Air India', 'Jet Airways', 'SpiceJet',\n",
        "       'Multiple carriers', 'GoAir', 'Vistara', 'Air Asia']:\n",
        "    New_Airline_List.append(carrier)\n",
        "  else:\n",
        "    New_Airline_List.append('Other')\n",
        "Airline['Airline'] = pd.DataFrame(New_Airline_List)\n",
        "Airline = pd.get_dummies(Airline, drop_first= True)\n",
        "\n",
        "# Feature Engineering on: 'Source'\n",
        "Source = unseen_dataset[[\"Source\"]]\n",
        "Source = pd.get_dummies(Source, drop_first= True)\n",
        "Source.head()\n",
        "\n",
        "# Feature Engineering on: 'Destination'\n",
        "Destination = unseen_dataset[[\"Destination\"]]\n",
        "Current_Destination_List = Destination['Destination']\n",
        "New_Destination_List = []\n",
        "for value in Current_Destination_List:\n",
        "  if value in ['New Delhi']:\n",
        "    New_Destination_List.append('Delhi')\n",
        "  else:\n",
        "    New_Destination_List.append(value)\n",
        "Destination['Destination'] = pd.DataFrame(New_Destination_List)\n",
        "Destination['Destination'].value_counts()\n",
        "Destination = pd.get_dummies(Destination, drop_first = True)\n",
        "Destination.head()\n",
        "\n",
        "# Feature Engineering on: 'Route', 'Additional_Info\n",
        "unseen_dataset.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n",
        "\n",
        "# Feature Engineering on: 'Total_Stops'\n",
        "unseen_dataset.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n",
        "\n",
        "# Concatenate dataframe --> train_data + Airline + Source + Destination\n",
        "data_test = pd.concat([unseen_dataset, Airline, Source, Destination], axis = 1)\n",
        "data_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n",
        "\n",
        "# See how the test dataset looks\n",
        "data_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMyw5gG_uY8n"
      },
      "outputs": [],
      "source": [
        "# Drop 'Source_Delhi'\n",
        "X_unseen = data_test.loc[:, ['Total_Stops', 'journey_day', 'journey_month', 'dep_hour',\n",
        "       'dep_min', 'arrival_hour', 'arrival_min', 'Duration_hours',\n",
        "       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n",
        "       'Airline_Jet Airways', 'Airline_Multiple carriers', 'Airline_Other',\n",
        "       'Airline_SpiceJet', 'Airline_Vistara', 'Source_Chennai',\n",
        "       'Source_Kolkata', 'Source_Mumbai', 'Destination_Cochin',\n",
        "       'Destination_Delhi', 'Destination_Hyderabad', 'Destination_Kolkata']]\n",
        "y_unseen = data_test.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkq9q2s_yGMG"
      },
      "outputs": [],
      "source": [
        "y_pred = rf_model.predict(X_unseen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S64OqbzF1Dt"
      },
      "outputs": [],
      "source": [
        "print('R2 value: ', round(metrics.r2_score(y_unseen, y_pred),2))\n",
        "print('Normalized RMSE: ', round(np.sqrt(metrics.mean_squared_error(y_unseen, y_pred))/(max(y_unseen)-min(y_unseen)),2))\n",
        "print('Max Value: ', max(y_unseen), '\\nMin Value: ', min(y_unseen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I3M-2O-yW3h"
      },
      "outputs": [],
      "source": [
        "# writing model output file\n",
        "df_y_pred = pd.DataFrame(y_pred,columns= ['Predicted Price'])\n",
        "original_dataset = pd.read_excel(\"./a2_Unseen_Dataset.xlsx\")\n",
        "dfx = pd.concat([original_dataset, df_y_pred], axis=1)\n",
        "dfx.to_excel(\"c2_ModelOutput.xlsx\")\n",
        "dfx.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWCd6yk9tvnB"
      },
      "source": [
        "---\n",
        "# In the next tutorial\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZdw_DXFS1Fm"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "\n",
        "* Choose following method for hyperparameter tuning\n",
        "    1. **RandomizedSearchCV** --> Fast\n",
        "    2. **GridSearchCV**\n",
        "* Assign hyperparameters in form of dictionary\n",
        "* Fit the model\n",
        "* Check best paramters and best score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhHmVC9jS1Fm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4foe6EnrS1Fm"
      },
      "outputs": [],
      "source": [
        "#Randomized Search CV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10, 15, 100]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 5, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P1i--5yS1Fm"
      },
      "outputs": [],
      "source": [
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sfhJeXpS1Fm"
      },
      "outputs": [],
      "source": [
        "# Random search of parameters, using 5 fold cross validation, \n",
        "# search across 100 different combinations\n",
        "rf_random = RandomizedSearchCV(estimator = rf_reg, param_distributions = random_grid,\n",
        "                               scoring='neg_mean_squared_error', n_iter = 10, cv = 5, \n",
        "                               verbose=2, random_state=42, n_jobs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6o4FoG2S1Fm"
      },
      "outputs": [],
      "source": [
        "# Model Training with Hyperparameter Tuning\n",
        "rf_random.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSbT_HsLS1Fm"
      },
      "outputs": [],
      "source": [
        "rf_random.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOsDYms0QrBY"
      },
      "outputs": [],
      "source": [
        "# Plot Performance Chart\n",
        "prediction = rf_random.predict(X_test)\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.scatter(y_test, prediction, alpha = 0.5)\n",
        "plt.xlabel(\"y_test\")\n",
        "plt.ylabel(\"y_pred\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxl579dsQtZt"
      },
      "outputs": [],
      "source": [
        "# RMSE/(max(DV)-min(DV))\n",
        "print('R2 value: ', round(metrics.r2_score(y_test, prediction),2))\n",
        "print('RMSE: ', round(np.sqrt(metrics.mean_squared_error(y_test, prediction)),2))\n",
        "print('Normalized RMSE: ', round(np.sqrt(metrics.mean_squared_error(y_test, prediction))/(max(y_test)-min(y_test)),2))\n",
        "print('Max Value: ', max(y_test), '\\nMin Value: ', min(y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}